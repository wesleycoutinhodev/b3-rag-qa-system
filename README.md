# B3 Q\&A com RAG ‚Äî Sistema de Perguntas e Respostas Aumentadas (RAG)

## üéØ Vis√£o Geral do Projeto

Este projeto implementa um sistema de Perguntas e Respostas (Q\&A) focado em documentos e informa√ß√µes da B3 e not√≠cias da √°rea economica, e de empresas da B3 (Bolsa de Valores do Brasil). Utiliza a arquitetura **Retrieval-Augmented Generation (RAG)** para combinar a capacidade de recupera√ß√£o de informa√ß√µes com o poder de racioc√≠nio de um Large Language Model (LLM).

O fluxo de trabalho inclui a ingest√£o de um *corpus* de documentos, o processamento de textos em *chunks*, a gera√ß√£o de **Embeddings**, o armazenamento em um √≠ndice vetorial (FAISS) e, por fim, a utiliza√ß√£o do LLM para gerar respostas contextualmente relevantes.

## üß† Detalhe do Uso de Intelig√™ncia Artificial (IA)

A IA √© a espinha dorsal deste projeto, sendo aplicada em duas etapas principais atrav√©s da arquitetura RAG:

1.  **Retrieval (Recupera√ß√£o):** A pergunta do usu√°rio √© convertida em um vetor (Embedding). Este vetor √© usado para buscar os trechos de texto (*chunks*) mais relevantes no √≠ndice FAISS.
2.  **Generation (Gera√ß√£o):** Os trechos de texto recuperados s√£o anexados √† pergunta original, formando um **prompt contextualizado**. Este prompt √© enviado ao Large Language Model (LLM) para que ele formule uma resposta coerente e baseada estritamente nas informa√ß√µes fornecidas pelo contexto.

O modelo de LLM utilizado para a gera√ß√£o √© o **Qwen2.5-7B-Instruct** (`Qwen/Qwen2.5-7B-Instruct`), um modelo otimizado para instru√ß√µes.

## üíª Uso de Hugging Face

A plataforma Hugging Face √© essencial, pois fornece os modelos de Machine Learning (ML) abertos e de alto desempenho utilizados no projeto:

  * **Large Language Model (LLM):** `Qwen/Qwen2.5-7B-Instruct`. √â o modelo respons√°vel por ler o contexto recuperado e gerar a resposta final ao usu√°rio.
  * **Modelo de Embedding:** `BAAI/bge-m3`. Este modelo multil√≠ngue e multifuncional √© usado para converter textos em vetores.
  * **Modelo de Re-ranker (Opcional):** `BAAI/bge-reranker-v2-m3`. Usado para refinar a qualidade dos documentos recuperados, reordenando os *chunks* mais relevantes antes de envi√°-los ao LLM, melhorando a precis√£o da resposta.

## üìä O Papel dos Embeddings (Vetores Sem√¢nticos)

**Embeddings** s√£o representa√ß√µes num√©ricas (vetores) de textos, onde a similaridade de significado entre os textos se traduz na proximidade matem√°tica dos seus vetores correspondentes.

1.  **Cria√ß√£o do Conhecimento:** Cada bloco de texto (chunk) do *corpus* da B3 √© transformado em um vetor (embedding) usando o modelo `BAAI/bge-m3`.
2.  **Indexa√ß√£o (FAISS):** Esses vetores s√£o armazenados e indexados no banco de dados vetorial **FAISS** (`faiss-cpu`). O FAISS permite a busca eficiente (em milissegundos) dos vetores que est√£o semanticamente mais pr√≥ximos do vetor da pergunta do usu√°rio.
3.  **Configura√ß√µes:** O processo de cria√ß√£o de embeddings utiliza o par√¢metro `CHUNK_TOKENS = 800` para segmentar o texto em blocos de tamanho ideal e `CHUNK_OVERLAP = 100` para garantir a continuidade do contexto entre os blocos.

## üõ†Ô∏è Configura√ß√£o e Instala√ß√£o

### 1\. Depend√™ncias

Instale as bibliotecas necess√°rias para o processamento de texto, cria√ß√£o de embeddings, busca vetorial e execu√ß√£o do LLM:

```bash
!pip -q install transformers accelerate bitsandbytes sentence-transformers faiss-cpu unstructured pdfminer.six pypdf tiktoken tqdm gdown
```

### 2\. Par√¢metros-chave

Os principais par√¢metros de configura√ß√£o do sistema est√£o definidos no notebook:

| Par√¢metro | Valor Padr√£o | Descri√ß√£o |
| :--- | :--- | :--- |
| **`MODEL_LLM`** | `Qwen/Qwen2.5-7B-Instruct` | O Large Language Model utilizado para gera√ß√£o.|
| **`EMB_MODEL`** | `BAAI/bge-m3` | O modelo da Hugging Face para gerar Embeddings. |
| **`RERANK_MODEL`** | `BAAI/bge-reranker-v2-m3` | O modelo para reordenar os documentos recuperados. |
| **`CHUNK_TOKENS`** | `800` | Tamanho m√°ximo de tokens por bloco de texto. |
| **`TOPN`** | `5` | Quantidade de *chunks* (contexto) final que √© enviado ao LLM. |
| **`RESTORE_FROM_DRIVE`** | `True` | Se **True**, pula o processamento de chunking/embeddings e baixa os artefatos prontos do Google Drive. |

### 3\. Execu√ß√£o

O projeto √© executado sequencialmente atrav√©s das c√©lulas do notebook:

1.  **Instala√ß√£o de Depend√™ncias:** Garante que todas as bibliotecas est√£o instaladas.
2.  **Configura√ß√µes e Diret√≥rios:** Define modelos e par√¢metros.
3.  **Persist√™ncia via Google Drive:** Permite restaurar artefatos pr√©-processados (`chunks.jsonl`, `faiss.index`, `meta.jsonl`) para acelerar a inicializa√ß√£o (se `RESTORE_FROM_DRIVE=True`).
4.  **Chunking (Se necess√°rio):** Divide os documentos do *corpus* (`corpus.jsonl`) em peda√ßos menores (`chunks.jsonl`).
5.  **Embeddings + FAISS (Se necess√°rio):** Cria os vetores de cada *chunk* e constr√≥i o √≠ndice FAISS para busca vetorial.
6.  **Busca e Reranker:** Carrega os modelos e o √≠ndice, e define as fun√ß√µes de busca.
7.  **LLM Inference:** Carrega o modelo de gera√ß√£o e a fun√ß√£o final (`answer`) para responder √†s perguntas usando o contexto recuperado.
