{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wesleycoutinhodev/b3-rag-qa-system/blob/main/v3_of_B3_RAG_QA_Colab_Drive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-tU_kZFJCLzO",
      "metadata": {
        "id": "-tU_kZFJCLzO"
      },
      "source": [
        "# B3 Q&A com RAG â€” versÃ£o com persistÃªncia no Google Drive\n",
        "\n",
        "Inclui cÃ©lulas para **baixar/restaurar** artefatos (corpus/chunks/meta/FAISS) de um **.zip pÃºblico** no Google Drive e para **salvar/atualizar** o pacote apÃ³s novo processamento."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49tiLvaFCLzS",
      "metadata": {
        "id": "49tiLvaFCLzS"
      },
      "source": [
        "## 1) InstalaÃ§Ã£o de dependÃªncias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "utsjT7yACLzT",
      "metadata": {
        "id": "utsjT7yACLzT"
      },
      "outputs": [],
      "source": [
        "!pip -q install transformers accelerate bitsandbytes sentence-transformers faiss-cpu unstructured pdfminer.six pypdf tiktoken tqdm gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "om3ATOYKCLzU",
      "metadata": {
        "id": "om3ATOYKCLzU"
      },
      "source": [
        "## 2) ConfiguraÃ§Ãµes e diretÃ³rios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-lmSNFziCLzU",
      "metadata": {
        "id": "-lmSNFziCLzU"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "DATA_DIR = Path(\"/content/data\")\n",
        "INDEX_DIR = Path(\"/content/index\")\n",
        "EVAL_DIR = Path(\"/content/eval\")\n",
        "for p in [DATA_DIR, INDEX_DIR, EVAL_DIR]:\n",
        "    p.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "MODEL_LLM = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "EMB_MODEL = \"BAAI/bge-m3\"\n",
        "RERANK_MODEL = \"BAAI/bge-reranker-v2-m3\"\n",
        "\n",
        "CHUNK_TOKENS = 800\n",
        "CHUNK_OVERLAP = 100\n",
        "TOPK = 20\n",
        "TOPN = 5\n",
        "MAX_NEW_TOKENS = 450\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# âœ… FLAG DE CONTROLE\n",
        "# True  = Baixar e restaurar Ã­ndice PRONTO do Drive (pula chunking/embedding).\n",
        "# False = Processar do zero a partir do corpus.jsonl (executa chunking/embedding).\n",
        "# -----------------------------------------------------------------\n",
        "RESTORE_FROM_DRIVE = True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7Z40FfjOCLzU",
      "metadata": {
        "id": "7Z40FfjOCLzU"
      },
      "source": [
        "## 3) PersistÃªncia via Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kF9936gmGubI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kF9936gmGubI",
        "outputId": "54ede95c-2a8d-4820-828b-01ebf66c4072"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to retrieve file url:\n",
            "\n",
            "\tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses.\n",
            "\tCheck FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.\n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\thttps://drive.google.com/uc?id=https_drive.google.com/uc?id=1dmHUXwes4rpGW9XAYCJUUKv5HRvgRgkl\n",
            "\n",
            "but Gdown can't. Please check connections and permissions.\n",
            "âŒ Falha ao baixar o corpus de file_id: 1dmHUXwes4rpGW9XAYCJUUKv5HRvgRgkl\n"
          ]
        }
      ],
      "source": [
        "# ðŸ”½ Baixar/restaurar CORPUS pronto (JSONL) do Drive pÃºblico\n",
        "from pathlib import Path\n",
        "import os, json\n",
        "\n",
        "CORPUS_FILE_ID = \"1dmHUXwes4rpGW9XAYCJUUKv5HRvgRgkl\"\n",
        "CORPUS_JSONL_PATH = DATA_DIR / \"corpus.jsonl\"\n",
        "\n",
        "def download_corpus_to_data_dir(file_id: str, dest_path: Path) -> bool:\n",
        "\n",
        "    # Garante que o diretÃ³rio de destino existe\n",
        "    dest_path.parent.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    url = f\"https_drive.google.com/uc?id={file_id}\" # (corrigido para evitar link)\n",
        "\n",
        "    # âœ… Baixa diretamente para o caminho final\n",
        "    !gdown --fuzzy \"{url}\" -O \"{str(dest_path)}\"\n",
        "\n",
        "    ok = dest_path.exists() and dest_path.stat().st_size > 0\n",
        "    if ok:\n",
        "        print(f\"âœ… Corpus baixado com sucesso para: {dest_path}\")\n",
        "    else:\n",
        "        print(f\"âŒ Falha ao baixar o corpus de file_id: {file_id}\")\n",
        "    return ok\n",
        "\n",
        "corpus_downloaded = download_corpus_to_data_dir(CORPUS_FILE_ID, CORPUS_JSONL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xvlouYtXCLzU",
      "metadata": {
        "id": "xvlouYtXCLzU"
      },
      "outputs": [],
      "source": [
        "# ðŸ”½ Baixar/restaurar artefatos de um ZIP pÃºblico no Drive\n",
        "\n",
        "if RESTORE_FROM_DRIVE:\n",
        "    print(\"Iniciando restauraÃ§Ã£o dos artefatos (index, meta, chunks) do Drive...\")\n",
        "    FILE_ID = \"1zqTf2D3DvYooqy0iqnUb32kUIOtzJd8f\"\n",
        "    ZIP_DEST = \"/content/b3_rag_artifacts.zip\"\n",
        "\n",
        "    def restore_from_drive_public(file_id: str):\n",
        "        if not file_id:\n",
        "            print(\"Defina FILE_ID para tentar baixar do Drive pÃºblico.\")\n",
        "            return False\n",
        "        !gdown --id $file_id -O $ZIP_DEST\n",
        "        # Use a flag -o (overwrite) e -j (junk paths)\n",
        "        !unzip -o -j /content/b3_rag_artifacts.zip -d /content/index/\n",
        "\n",
        "        # Limpa o zip baixado\n",
        "        if os.path.exists(ZIP_DEST):\n",
        "            os.remove(ZIP_DEST)\n",
        "\n",
        "        return (INDEX_DIR/\"faiss.index\").exists() and (INDEX_DIR/\"meta.jsonl\").exists()\n",
        "\n",
        "    restored = restore_from_drive_public(FILE_ID)\n",
        "    print(\"Restaurado do Drive pÃºblico âœ…\" if restored else \"âŒ Falha ao restaurar do Drive.\")\n",
        "else:\n",
        "    print(\"Pulando restauraÃ§Ã£o de artefatos (RESTORE_FROM_DRIVE=False).\")\n",
        "    restored = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2I7JbF0SCLzV",
      "metadata": {
        "id": "2I7JbF0SCLzV"
      },
      "outputs": [],
      "source": [
        "# â¬†ï¸ Salvar/atualizar artefatos no seu Drive (minha conta)\n",
        "SAVE_PATH = \"/content/drive/MyDrive/b3_rag_public/b3_rag_artifacts.zip\"\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "#!mkdir -p /content/drive/MyDrive/b3_rag_public\n",
        "\n",
        "#print(f\"Salvando artefatos de {INDEX_DIR} para {SAVE_PATH}...\")\n",
        "#!zip -r $SAVE_PATH $INDEX_DIR\n",
        "#print(\"ZIP atualizado em:\", SAVE_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A-F9saitCLzW",
      "metadata": {
        "id": "A-F9saitCLzW"
      },
      "source": [
        "## 5) Chunking (pule se jÃ¡ restaurou)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48ErZj8WCLzW",
      "metadata": {
        "id": "48ErZj8WCLzW"
      },
      "outputs": [],
      "source": [
        "if not RESTORE_FROM_DRIVE:\n",
        "    print(\"Iniciando processo de Chunking (RESTORE_FROM_DRIVE=False)...\")\n",
        "    import tiktoken, json, re, gc\n",
        "    from pathlib import Path\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    # Tokenizador\n",
        "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "    def ntokens(s: str) -> int:\n",
        "        try:\n",
        "            return len(enc.encode(s))\n",
        "        except Exception:\n",
        "            return len(s.split())\n",
        "\n",
        "    def chunk_text(text: str, chunk_tokens=CHUNK_TOKENS, overlap_tokens=CHUNK_OVERLAP):\n",
        "        paras = [p.strip() for p in re.split(r\"\\n\\s*\\n\", text) if p.strip()]\n",
        "        cur = \"\"\n",
        "        # emite um chunk por vez\n",
        "        for p in paras:\n",
        "            if ntokens(cur) + ntokens(p) <= chunk_tokens:\n",
        "                cur += (\"\\n\\n\" + p) if cur else p\n",
        "            else:\n",
        "                if cur:\n",
        "                    yield cur\n",
        "                tail = \" \".join(cur.split()[-overlap_tokens:]) if cur else \"\"\n",
        "                cur = (tail + \" \" + p).strip()\n",
        "        if cur:\n",
        "            yield cur\n",
        "\n",
        "    # âœ… Garante que o input path Ã© o correto\n",
        "    in_path  = DATA_DIR / \"corpus.jsonl\"\n",
        "    meta_out = INDEX_DIR / \"meta.jsonl\"\n",
        "    chunk_out = INDEX_DIR / \"chunks.jsonl\"\n",
        "\n",
        "    # Zera arquivos de saÃ­da\n",
        "    meta_out.write_text(\"\", encoding=\"utf-8\")\n",
        "    chunk_out.write_text(\"\", encoding=\"utf-8\")\n",
        "\n",
        "    total_docs_est = sum(1 for _ in open(in_path, encoding=\"utf-8\"))\n",
        "    print(f\"ðŸ“„ Corpus com {total_docs_est:,} documentos\\n\")\n",
        "\n",
        "    total_docs = 0\n",
        "    total_chunks = 0\n",
        "\n",
        "    with open(in_path, \"r\", encoding=\"utf-8\") as fin, \\\n",
        "         open(meta_out, \"a\", encoding=\"utf-8\") as fmeta, \\\n",
        "         open(chunk_out, \"a\", encoding=\"utf-8\") as fchunk:\n",
        "\n",
        "        for line in tqdm(fin, total=total_docs_est, desc=\"ðŸ”¹ Chunking\", unit=\"doc\"):\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            d = json.loads(line)\n",
        "            total_docs += 1\n",
        "            chunk_id = 0\n",
        "            for ch in chunk_text(d[\"text\"], CHUNK_TOKENS, CHUNK_OVERLAP):\n",
        "                fmeta.write(json.dumps({\n",
        "                    \"doc_id\": d[\"id\"],\n",
        "                    \"title\": d[\"title\"],\n",
        "                    \"url\": d.get(\"url\",\"\"),\n",
        "                    \"chunk_id\": chunk_id\n",
        "                }, ensure_ascii=False) + \"\\n\")\n",
        "                fchunk.write(json.dumps({\"text\": ch}, ensure_ascii=False) + \"\\n\")\n",
        "                total_chunks += 1\n",
        "                chunk_id += 1\n",
        "\n",
        "            del d\n",
        "            gc.collect()\n",
        "\n",
        "    print(f\"\\nâœ… Finalizado: {total_docs:,} documentos processados, {total_chunks:,} chunks gerados.\")\n",
        "else:\n",
        "    print(\"Pulando Chunking (RESTORE_FROM_DRIVE=True).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XGnrmNO2CLzW",
      "metadata": {
        "id": "XGnrmNO2CLzW"
      },
      "source": [
        "## 6) Embeddings + FAISS (pule se jÃ¡ restaurou)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g_TZRrsSCLzW",
      "metadata": {
        "id": "g_TZRrsSCLzW"
      },
      "outputs": [],
      "source": [
        "if not RESTORE_FROM_DRIVE:\n",
        "    print(\"Iniciando processo de Embeddings e FAISS (RESTORE_FROM_DRIVE=False)...\")\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    import numpy as np, faiss, json\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    emb = SentenceTransformer(EMB_MODEL)\n",
        "    chunks = [json.loads(l)[\"text\"] for l in open(INDEX_DIR/\"chunks.jsonl\", encoding=\"utf-8\")]\n",
        "    meta = [json.loads(l) for l in open(INDEX_DIR/\"meta.jsonl\", encoding=\"utf-8\")]\n",
        "\n",
        "    vecs = []\n",
        "    for ch in tqdm(chunks, desc=\"Embeddings\"):\n",
        "        v = emb.encode(ch, convert_to_numpy=True, normalize_embeddings=True)\n",
        "        vecs.append(v.astype(\"float32\"))\n",
        "\n",
        "    vectors = np.vstack(vecs).astype(\"float32\")\n",
        "\n",
        "    index = faiss.IndexFlatIP(vectors.shape[1])\n",
        "    index.add(vectors)\n",
        "    faiss.write_index(index, str(INDEX_DIR/\"faiss.index\"))\n",
        "    print(\"Index size:\", index.ntotal)\n",
        "else:\n",
        "    print(\"Pulando Embeddings/FAISS (RESTORE_FROM_DRIVE=True).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W0tCxufPCLzW",
      "metadata": {
        "id": "W0tCxufPCLzW"
      },
      "source": [
        "## 7) Busca e (opcional) reranker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89Mdx4TDCLzW",
      "metadata": {
        "id": "89Mdx4TDCLzW"
      },
      "outputs": [],
      "source": [
        "import json, numpy as np, faiss\n",
        "index = faiss.read_index(str(INDEX_DIR/\"faiss.index\"))\n",
        "meta = [json.loads(l) for l in open(INDEX_DIR/\"meta.jsonl\", encoding=\"utf-8\")]\n",
        "chunks = [json.loads(l)[\"text\"] for l in open(INDEX_DIR/\"chunks.jsonl\", encoding=\"utf-8\")]\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "emb = SentenceTransformer(EMB_MODEL)\n",
        "\n",
        "def search(query: str, topk=TOPK):\n",
        "    qv = emb.encode(query, convert_to_numpy=True, normalize_embeddings=True).astype(\"float32\")[None,:]\n",
        "    D, I = index.search(qv, topk)\n",
        "    cands = [{\"text\": chunks[i], **meta[i], \"score\": float(D[0][j])} for j,i in enumerate(I[0])]\n",
        "    return cands\n",
        "\n",
        "try:\n",
        "    from sentence_transformers import CrossEncoder\n",
        "    reranker = CrossEncoder(RERANK_MODEL)\n",
        "    def rerank(query, cands, topn=TOPN):\n",
        "        pairs = [(query, c[\"text\"]) for c in cands]\n",
        "        scores = reranker.predict(pairs).tolist()\n",
        "        for c, s in zip(cands, scores):\n",
        "            c[\"rerank\"] = float(s)\n",
        "        return sorted(cands, key=lambda x: x[\"rerank\"], reverse=True)[:topn]\n",
        "except Exception as e:\n",
        "    print(\"[INFO] Reranker indisponÃ­vel:\", e)\n",
        "    reranker = None\n",
        "    def rerank(query, cands, topn=TOPN):\n",
        "        return cands[:topn]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EEHOCw62CLzW",
      "metadata": {
        "id": "EEHOCw62CLzW"
      },
      "source": [
        "## 8) LLM 4-bit e Q&A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a87ev9-jCLzW",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a87ev9-jCLzW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "print(\"Carregando LLM (Qwen) e Tokenizer...\")\n",
        "bnb = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_LLM)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_LLM, device_map=\"auto\", quantization_config=bnb\n",
        ")\n",
        "\n",
        "def build_prompt(question, ctx_items):\n",
        "    # âœ… LÃ³gica de prompt mantida\n",
        "    ctx_str = \"\\n\\n\".join(\n",
        "        [f\"[{i+1}] TÃ­tulo: {c['title']}\\nFonte: {c.get('url','')}\\nTrecho:\\n{c['text'][:1800]}\"\n",
        "         for i,c in enumerate(ctx_items)]\n",
        "    )\n",
        "    return (\n",
        "        \"VocÃª Ã© um assistente financeiro e responde sobre a B3.\\n\"\n",
        "        \"REGRAS: Use APENAS as informaÃ§Ãµes do CONTEXTO. Se faltar evidÃªncia, diga que nÃ£o sabe.\\n\"\n",
        "        \"Inclua referÃªncias [nÂº] e liste as fontes no final.\\n\\n\"\n",
        "        f\"PERGUNTA:\\n{question}\\n\\n\"\n",
        "        f\"CONTEXTO:\\n{ctx_str}\\n\\n\"\n",
        "        \"RESPOSTA:\"\n",
        "    )\n",
        "\n",
        "@torch.inference_mode()\n",
        "def answer(question, topk=TOPK, topn=TOPN, max_new_tokens=MAX_NEW_TOKENS, temperature=0.0):\n",
        "    # âœ… FunÃ§Ãµes 'search' e 'rerank' foram definidas na CÃ©lula 22\n",
        "    cands = search(question, topk=topk)\n",
        "    ctx = rerank(question, cands, topn=topn)\n",
        "    prompt = build_prompt(question, ctx)\n",
        "    input_ids = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    out_ids = model.generate(\n",
        "        **input_ids,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False if temperature==0 else True,\n",
        "        temperature=temperature,\n",
        "        pad_token_id=tok.eos_token_id\n",
        "    )\n",
        "    text = tok.decode(out_ids[0], skip_special_tokens=True)\n",
        "    resp = text.split(\"RESPOSTA:\")[-1].strip()\n",
        "    fontes = \"\\n\".join([f\"[{i+1}] {c['title']} â€” {c.get('url','')}\" for i,c in enumerate(ctx)])\n",
        "    return resp + \"\\n\\nFontes:\\n\" + fontes\n",
        "\n",
        "print(\"âœ… LLM e funÃ§Ã£o 'answer' prontos.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2DHyh2iTCLzX",
      "metadata": {
        "id": "2DHyh2iTCLzX"
      },
      "source": [
        "## 9) Teste rÃ¡pido"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Zcq8ojxKCLzX",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Zcq8ojxKCLzX"
      },
      "outputs": [],
      "source": [
        "print(answer(\"Como a taxa Selic afetou o mercado financeiro?\", topn=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3jLvrT0gddk3",
      "metadata": {
        "id": "3jLvrT0gddk3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "# --- Contexto Simulado (Valores reais viriam da execuÃ§Ã£o do seu RAG) ---\n",
        "# SimulaÃ§Ã£o do vetor de embedding da pergunta (dim=1024 para bge-m3)\n",
        "QUERY_EMBEDDING = np.random.rand(1, 1024).astype('float32')\n",
        "QUERY_EMBEDDING /= np.linalg.norm(QUERY_EMBEDDING) # NormalizaÃ§Ã£o!\n",
        "\n",
        "# SimulaÃ§Ã£o da busca FAISS (Normalmente o FAISS retorna D=DistÃ¢ncias e I=Ãndices)\n",
        "# D: Matriz de distÃ¢ncias (scores de similaridade). Como usamos IP e normalizamos, D Ã© o Cosseno.\n",
        "# I: Matriz de Ã­ndices (ID do chunk no metadados/corpus).\n",
        "D_SIMULADO = np.array([[0.98, 0.95, 0.92, 0.85, 0.70]]) # Cosseno alto = Alta similaridade\n",
        "I_SIMULADO = np.array([[25, 42, 18, 99, 5]]) # IDs dos chunks\n",
        "K_BUSCA = 5 # Top-K buscado\n",
        "\n",
        "# SimulaÃ§Ã£o dos metadados (para mapear o ID do chunk Ã  sua origem e texto)\n",
        "METADATA_CHUNKS = {\n",
        "    25: {\"texto_chunk\": \"A alta do Ibovespa foi impulsionada pelo fluxo estrangeiro...\", \"fonte\": \"Fonte A\"},\n",
        "    42: {\"texto_chunk\": \"O corte de juros nos EUA atraiu capital para mercados emergentes...\", \"fonte\": \"Fonte B\"},\n",
        "    18: {\"texto_chunk\": \"Empresas como Vale e Petrobras tiveram ganhos significativos...\", \"fonte\": \"Fonte A\"},\n",
        "    99: {\"texto_chunk\": \"O Ibovespa atingiu novo recorde em 2025...\", \"fonte\": \"Fonte C\"},\n",
        "    5: {\"texto_chunk\": \"A B3 lanÃ§a novo Ã­ndice de tÃ­tulos...\", \"fonte\": \"Fonte D\"},\n",
        "}\n",
        "# --- Fim do Contexto Simulado ---\n",
        "\n",
        "\n",
        "def extrair_e_analisar_faiss_scores(D, I, metadata, query_embedding):\n",
        "    \"\"\"\n",
        "    FunÃ§Ã£o para analisar e estruturar as mÃ©tricas FAISS recuperadas.\n",
        "\n",
        "    Argumentos:\n",
        "        D (np.array): Matriz de scores de similaridade (distÃ¢ncias).\n",
        "        I (np.array): Matriz de Ã­ndices dos chunks recuperados.\n",
        "        metadata (dict): DicionÃ¡rio de metadados para mapear o Ã­ndice ao conteÃºdo.\n",
        "        query_embedding (np.array): Embedding da pergunta.\n",
        "    \"\"\"\n",
        "    if D.shape[0] == 0:\n",
        "        print(\"Nenhum resultado encontrado pelo FAISS.\")\n",
        "        return []\n",
        "\n",
        "    # O FAISS retorna os scores, que, devido Ã  normalizaÃ§Ã£o, SÃƒO as DistÃ¢ncias de Cosseno.\n",
        "    scores_cosseno = D[0]\n",
        "    indices_chunks = I[0]\n",
        "\n",
        "    resultados_detalhados = []\n",
        "\n",
        "    print(f\"--- AnÃ¡lise da Busca FAISS (Top-{len(indices_chunks)}) ---\")\n",
        "    print(f\"DimensÃ£o do Embedding da Pergunta: {query_embedding.shape[1]}\")\n",
        "    print(f\"Tipo de Similaridade: Produto Interno (IP) em Vetores Normalizados = DistÃ¢ncia de Cosseno\\n\")\n",
        "\n",
        "    for rank, (score, chunk_id) in enumerate(zip(scores_cosseno, indices_chunks)):\n",
        "        # Obter os metadados do chunk\n",
        "        chunk_info = metadata.get(chunk_id, {\"texto_chunk\": \"Chunk nÃ£o encontrado\", \"fonte\": \"N/A\"})\n",
        "\n",
        "        resultado = {\n",
        "            \"rank\": rank + 1,\n",
        "            \"score_cosseno\": float(score),\n",
        "            \"chunk_id\": int(chunk_id),\n",
        "            \"fonte\": chunk_info['fonte'],\n",
        "            \"inicio_do_chunk\": chunk_info['texto_chunk'][:100] + \"...\"\n",
        "        }\n",
        "        resultados_detalhados.append(resultado)\n",
        "\n",
        "        # Imprimir o resumo\n",
        "        print(f\"[{rank + 1}Âº] Score de Cosseno: {score:.4f} (ID: {chunk_id})\")\n",
        "        print(f\"    Fonte: {chunk_info['fonte']}\")\n",
        "        print(f\"    InÃ­cio do Texto: {chunk_info['texto_chunk'][:80]}...\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "    # MÃ©trica Agregada: MÃ©dia do Top-K\n",
        "    print(f\"\\nMÃ©dia de Similaridade do Top-{len(indices_chunks)}: {np.mean(scores_cosseno):.4f}\")\n",
        "\n",
        "    return resultados_detalhados\n",
        "\n",
        "# ExecuÃ§Ã£o da anÃ¡lise (usando dados simulados)\n",
        "analise_faiss = extrair_e_analisar_faiss_scores(D_SIMULADO, I_SIMULADO, METADATA_CHUNKS, QUERY_EMBEDDING)\n",
        "\n",
        "# O output 'analise_faiss' pode ser usado para depuraÃ§Ã£o ou para alimentar o Reranker (se ativo)\n",
        "# print(analise_faiss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8PqZHxRunxS7",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8PqZHxRunxS7"
      },
      "outputs": [],
      "source": [
        "# Lista de perguntas para o teste final\n",
        "perguntas_teste = [\n",
        "    \"O que impulsionou a alta do Ibovespa?\",\n",
        "    \"Como a taxa Selic afetou o mercado financeiro?\",\n",
        "    \"Quais setores apresentaram crescimento na bolsa?\",\n",
        "    \"Ã‰ um bom momento para investir em aÃ§Ãµes?\",\n",
        "    \"O que se espera do plano estratÃ©gico da Petrobras?\"\n",
        "]\n",
        "\n",
        "print(\"Iniciando Teste Final com\", len(perguntas_teste), \"perguntas...\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Executa cada pergunta\n",
        "for i, pergunta in enumerate(perguntas_teste):\n",
        "    print(f\"\\n--- TESTE {i+1}: {pergunta} ---\\n\")\n",
        "    try:\n",
        "        # Chama a funÃ§Ã£o 'answer' definida na cÃ©lula do LLM\n",
        "        resposta = answer(pergunta, topn=4) # Usando topn=4 como no seu exemplo\n",
        "        print(resposta)\n",
        "    except Exception as e:\n",
        "        print(f\"Ocorreu um erro ao processar a pergunta: {e}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "print(\"\\nâœ… Teste final concluÃ­do.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}